ams:
  admin-username: admin
  admin-password: admin
  server-bind-host: "0.0.0.0"
  server-expose-host: "127.0.0.1"

  thrift-server:
    max-message-size: 104857600 # 100MB
    selector-thread-count: 2
    selector-queue-size: 4
    table-service:
      bind-port: 1260
      worker-thread-count: 20
    optimizing-service:
      bind-port: 1261

  http-server:
    bind-port: 1630

  refresh-external-catalogs:
    interval: 180000 # 3min
    thread-count: 10
    queue-size: 1000000

  refresh-tables:
    thread-count: 10
    interval: 60000 # 1min

  self-optimizing:
    commit-thread-count: 10

  optimizer:
    heart-beat-timeout: 60000 # 1min
    task-ack-timeout: 30000 # 30s

  blocker:
    timeout: 60000 # 1min

  # optional features
  expire-snapshots:
    enabled: true
    thread-count: 10

  clean-orphan-files:
    enabled: true
    thread-count: 10

  sync-hive-tables:
    enabled: true
    thread-count: 10

  data-expiration:
    enabled: false
    thread-count: 10
    interval: 1d

  auto-create-tags:
    enabled: true
    thread-count: 3
    interval: 60000 # 1min

  database:
    type: derby
    jdbc-driver-class: org.apache.derby.jdbc.EmbeddedDriver
    url: jdbc:derby:/tmp/amoro/derby;create=true

  #    MySQL database configuration.
  #    database:
  #      type: mysql
  #      jdbc-driver-class: com.mysql.cj.jdbc.Driver
  #      url: jdbc:mysql://127.0.0.1:3306/db?useUnicode=true&characterEncoding=UTF8&autoReconnect=true&useAffectedRows=true&useSSL=false
  #      username: root
  #      password: root

  #  Postgres database configuration.
  #  database:
  #    type: postgres
  #    jdbc-driver-class: org.postgresql.Driver
  #    url: jdbc:postgresql://127.0.0.1:5432/db
  #    username: user
  #    password: passwd

  terminal:
    backend: local
    local.spark.sql.iceberg.handle-timestamp-without-timezone: false

#  Kyuubi terminal backend configuration.
#  terminal:
#    backend: kyuubi
#    kyuubi.jdbc.url: jdbc:hive2://127.0.0.1:10009/


#  High availability configuration.
#  ha:
#    enabled: true
#    cluster-name: default
#    zookeeper-address: 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183

containers:
  - name: localContainer
    container-impl: com.netease.arctic.server.manager.LocalOptimizerContainer
    properties:
      export.JAVA_HOME: "/opt/java"   # JDK environment

#containers:
#  - name: flinkContainer
#    container-impl: com.netease.arctic.server.manager.FlinkOptimizerContainer
#    properties:
#      flink-home: /opt/flink/                                     # Flink install home
#      target: yarn-per-job                                        # Flink run target, (yarn-per-job, yarn-application, kubernetes-application)
#      export.JVM_ARGS: -Djava.security.krb5.conf=/opt/krb5.conf   # Flink launch jvm args, like kerberos config when ues kerberos
#      export.HADOOP_CONF_DIR: /etc/hadoop/conf/                   # Hadoop config dir
#      export.HADOOP_USER_NAME: hadoop                             # Hadoop user submit on yarn
#      export.FLINK_CONF_DIR: /opt/flink/conf/                     # Flink config dir
#      # flink kubernetes application properties.
#      job-uri: "local:///opt/flink/usrlib/optimizer-job.jar"      # Optimizer job main jar for kubernetes application
#      flink-conf.kubernetes.container.image: "arctic163/optimizer-flink:{version}"   # Optimizer image ref
#      flink-conf.kubernetes.service-account: flink                # Service account that is used within kubernetes cluster.
